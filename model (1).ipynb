{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1vqqU1ZHhYwmuCShfdy050NXRTB3f-PfV","timestamp":1665037724079}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KLEoSsVEGzUL","outputId":"700e42e3-6a0e-4332-d85a-cd30b27ee2ec","executionInfo":{"status":"ok","timestamp":1669960904971,"user_tz":-330,"elapsed":4558,"user":{"displayName":"Agrima Malhotra","userId":"02024143427081502811"}}},"source":["!pip install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.25.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.11.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.9.24)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n"]}]},{"cell_type":"code","metadata":{"id":"N2R7JQdHFGBz","executionInfo":{"status":"ok","timestamp":1669975505534,"user_tz":-330,"elapsed":2291,"user":{"displayName":"Agrima Malhotra","userId":"02024143427081502811"}}},"source":["import torch\n","import torch.nn as nn\n","import pandas as pd\n","import numpy as np\n","import shutil\n","import sys   "],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"qMlPQF13FF9P","executionInfo":{"status":"ok","timestamp":1669975510623,"user_tz":-330,"elapsed":633,"user":{"displayName":"Agrima Malhotra","userId":"02024143427081502811"}}},"source":["url = 'https://raw.githubusercontent.com/akaAgrima/capstone/main/dataset.csv'\n","tdf = pd.read_csv(url, index_col=0)"],"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Filter out records ( values in clean_body and tags) that have atleast one of the top tags\n","\n","x=tdf['Body'] # To store the filtered clean_body values\n","y=tdf['tags'] # to store the corresponding tags"],"metadata":{"id":"2efiC9HLAvZr","executionInfo":{"status":"ok","timestamp":1669975511092,"user_tz":-330,"elapsed":10,"user":{"displayName":"Agrima Malhotra","userId":"02024143427081502811"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["import ast\n","for i in range(len(y)):\n","  res = ast.literal_eval(y[i])\n","  y[i]=tuple(res)"],"metadata":{"id":"khkHoDYqPx3X","executionInfo":{"status":"ok","timestamp":1669975511094,"user_tz":-330,"elapsed":10,"user":{"displayName":"Agrima Malhotra","userId":"02024143427081502811"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# Encode the tags(labels) in a binary format in order to be used for training\n","\n","from sklearn.preprocessing import MultiLabelBinarizer\n","mlb = MultiLabelBinarizer()\n"," \n","yt = mlb.fit_transform(y) \n"],"metadata":{"id":"fLHYZ6JhIUY_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#yt\n","mlb.classes_"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NIZD8NBSUyG9","executionInfo":{"status":"ok","timestamp":1669960907154,"user_tz":-330,"elapsed":41,"user":{"displayName":"Agrima Malhotra","userId":"02024143427081502811"}},"outputId":"c7548f59-5864-4827-d1d0-81134ae84e21"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['1', '10', '11', '12', '13', '13-A', '13-B', '14', '15', '16',\n","       '17', '17-A', '18', '19', '2', '20', '21', '21-A', '21-B', '22',\n","       '23', '23-A', '24', '25', '26', '27', '28', '28-A', '29', '3',\n","       '30', '4', '5', '6', '7', '8', '9'], dtype=object)"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["df = pd.DataFrame(yt, columns=mlb.classes_ )\n","df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"pvT8nbxnPJ8a","executionInfo":{"status":"ok","timestamp":1669960907155,"user_tz":-330,"elapsed":39,"user":{"displayName":"Agrima Malhotra","userId":"02024143427081502811"}},"outputId":"535815c7-3941-4c27-af33-6b4e87984ae2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["     1  10  11  12  13  13-A  13-B  14  15  16  ...  28-A  29  3  30  4  5  6  \\\n","0    0   0   0   1   0     0     0   0   0   0  ...     0   0  0   0  0  0  0   \n","1    0   0   0   0   0     0     0   0   0   0  ...     0   0  1   0  1  1  0   \n","2    0   0   0   0   0     0     1   0   0   0  ...     0   0  0   0  0  0  0   \n","3    0   0   0   0   0     0     0   0   0   0  ...     0   0  0   0  1  0  0   \n","4    0   0   1   0   0     0     0   0   0   1  ...     0   0  1   0  0  1  0   \n","..  ..  ..  ..  ..  ..   ...   ...  ..  ..  ..  ...   ...  .. ..  .. .. .. ..   \n","447  0   0   0   0   0     0     0   0   0   0  ...     0   0  0   0  0  0  0   \n","448  0   0   1   0   0     0     0   0   0   0  ...     0   0  0   0  0  0  1   \n","449  0   1   0   0   1     0     0   0   0   0  ...     0   0  0   0  0  0  0   \n","450  0   0   0   0   0     0     0   0   0   0  ...     0   0  0   0  0  0  0   \n","451  0   1   0   0   0     0     0   0   0   0  ...     0   0  0   0  0  0  0   \n","\n","     7  8  9  \n","0    0  0  0  \n","1    0  0  1  \n","2    0  0  0  \n","3    0  0  0  \n","4    0  0  0  \n","..  .. .. ..  \n","447  0  0  0  \n","448  0  0  0  \n","449  0  0  0  \n","450  0  0  0  \n","451  0  0  0  \n","\n","[452 rows x 37 columns]"],"text/html":["\n","  <div id=\"df-065534c9-1677-4bbe-801b-da74d3d9249a\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>1</th>\n","      <th>10</th>\n","      <th>11</th>\n","      <th>12</th>\n","      <th>13</th>\n","      <th>13-A</th>\n","      <th>13-B</th>\n","      <th>14</th>\n","      <th>15</th>\n","      <th>16</th>\n","      <th>...</th>\n","      <th>28-A</th>\n","      <th>29</th>\n","      <th>3</th>\n","      <th>30</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>447</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>448</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>449</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>450</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>451</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>452 rows Ã— 37 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-065534c9-1677-4bbe-801b-da74d3d9249a')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-065534c9-1677-4bbe-801b-da74d3d9249a button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-065534c9-1677-4bbe-801b-da74d3d9249a');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["import re\n","def pre_process(text):\n","\n","  # fetch alphabetic characters\n","  text = re.sub(\"[^a-zA-Z]\", \" \", text)\n","\n","  # convert text to lower case\n","  text = text.lower()\n","\n","  # split text into tokens to remove whitespaces\n","  tokens = text.split()\n","\n","  return \" \".join(tokens)"],"metadata":{"id":"TSfLbJKWiZtk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# clean the text in Body column\n","tdf['Body'] = tdf['Body'].apply(pre_process)"],"metadata":{"id":"09CuqaxCinMn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#see merge\n","train_df= pd.concat([tdf['Body'],df],axis=1)\n","#train_df"],"metadata":{"id":"qFSZmCa8KlDX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["columns = ['1','2','3','4','5','6','7','8','9','10','11','12','13A','13B','14','15','16','17','18','19','20','21','21A','21B','21C','22','23','23A','24','25','26','27','28','28A','29']"],"metadata":{"id":"GOBbqw8VOpWc"}},{"cell_type":"code","metadata":{"id":"5nY-51AnIFd5"},"source":["#train_df.columns"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OIt0M-mTIUaU"},"source":["#train_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UkrExSqJY3ma"},"source":["target_list = ['1', '10', '11', '12', '13', '13-A', '13-B', '14', '15', '16',\n","       '17', '17-A', '18', '19', '2', '20', '21', '21-A', '21-B', '22',\n","       '23', '23-A', '24', '25', '26', '27', '28', '28-A', '29', '3',\n","       '30', '4', '5', '6', '7', '8', '9']\n","target=len(target_list)\n","#print(target)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qSjmKyYJIl2M"},"source":["# hyperparameters\n","MAX_LEN = 300\n","TRAIN_BATCH_SIZE = 16\n","VALID_BATCH_SIZE = 16\n","EPOCHS = 50\n","LEARNING_RATE = 0.0001"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YMpQi7MpRPVb"},"source":["from transformers import BertTokenizer, BertModel"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ATDXZ45xIlyq"},"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2YXXbsj0Iltp"},"source":["class CustomDataset(torch.utils.data.Dataset):\n","\n","    def __init__(self, df, tokenizer, max_len):\n","        self.tokenizer = tokenizer\n","        self.df = df\n","        self.title = df['Body']\n","        self.targets = self.df[target_list].values\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.title)\n","\n","    def __getitem__(self, index):\n","        title = str(self.title[index])\n","        title = \" \".join(title.split())\n","\n","        inputs = self.tokenizer.encode_plus(\n","            title,\n","            None,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            padding='max_length',\n","            return_token_type_ids=True,\n","            truncation=True,\n","            return_attention_mask=True,\n","            return_tensors='pt'\n","        )\n","\n","        return {\n","            'input_ids': inputs['input_ids'].flatten(),\n","            'attention_mask': inputs['attention_mask'].flatten(),\n","            'token_type_ids': inputs[\"token_type_ids\"].flatten(),\n","            'targets': torch.FloatTensor(self.targets[index])\n","        }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IHyztNfdIlrJ"},"source":["train_size = 0.8\n","train_df = train_df.sample(frac=train_size, random_state=200).reset_index(drop=True)\n","val_df = train_df.drop(train_df.index).reset_index(drop=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3uOisCRwZusM"},"source":["train_dataset = CustomDataset(train_df, tokenizer, MAX_LEN)\n","valid_dataset = CustomDataset(val_df, tokenizer, MAX_LEN)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ilHjQp6RSnsw"},"source":["train_data_loader = torch.utils.data.DataLoader(train_dataset, \n","    batch_size=TRAIN_BATCH_SIZE,\n","    shuffle=True,\n","    num_workers=0\n",")\n","\n","val_data_loader = torch.utils.data.DataLoader(valid_dataset, \n","    batch_size=VALID_BATCH_SIZE,\n","    shuffle=False,\n","    num_workers=0\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uRUtQwS5Snqa"},"source":["device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ye-qQhzCWglF"},"source":["def load_ckp(checkpoint_fpath, model, optimizer):\n","    \"\"\"\n","    checkpoint_path: path to save checkpoint\n","    model: model that we want to load checkpoint parameters into       \n","    optimizer: optimizer we defined in previous training\n","    \"\"\"\n","    # load check point\n","    checkpoint = torch.load(checkpoint_fpath)\n","    # initialize state_dict from checkpoint to model\n","    model.load_state_dict(checkpoint['state_dict'])\n","    # initialize optimizer from checkpoint to optimizer\n","    optimizer.load_state_dict(checkpoint['optimizer'])\n","    # initialize valid_loss_min from checkpoint to valid_loss_min\n","    valid_loss_min = checkpoint['valid_loss_min']\n","    # return model, optimizer, epoch value, min validation loss \n","    return model, optimizer, checkpoint['epoch'], valid_loss_min.item()\n","\n","def save_ckp(state, is_best, checkpoint_path, best_model_path):\n","    \"\"\"\n","    state: checkpoint we want to save\n","    is_best: is this the best checkpoint; min validation loss\n","    checkpoint_path: path to save checkpoint\n","    best_model_path: path to save best model\n","    \"\"\"\n","    f_path = checkpoint_path\n","    # save checkpoint data to the path given, checkpoint_path\n","    torch.save(state, f_path)\n","    # if it is a best model, min validation loss\n","    if is_best:\n","        best_fpath = best_model_path\n","        # copy that checkpoint file to best path given, best_model_path\n","        shutil.copyfile(f_path, best_fpath)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fGw1dRqwSnlK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669960916225,"user_tz":-330,"elapsed":4375,"user":{"displayName":"Agrima Malhotra","userId":"02024143427081502811"}},"outputId":"eeccb65b-5bc8-4175-a224-5ee9f61690f7"},"source":["class BERTClass(torch.nn.Module):\n","    def __init__(self):\n","        super(BERTClass, self).__init__()\n","        self.bert_model = BertModel.from_pretrained('bert-base-uncased', return_dict=True)\n","        self.dropout = torch.nn.Dropout(0.3)\n","        self.linear = torch.nn.Linear(768, 37)\n","    \n","    def forward(self, input_ids, attn_mask, token_type_ids):\n","        output = self.bert_model(\n","            input_ids, \n","            attention_mask=attn_mask, \n","            token_type_ids=token_type_ids\n","        )\n","        output_dropout = self.dropout(output.pooler_output)\n","        output = self.linear(output_dropout)\n","        return output\n","\n","model = BERTClass()\n","model.to(device)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"execute_result","data":{"text/plain":["BERTClass(\n","  (bert_model): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.3, inplace=False)\n","  (linear): Linear(in_features=768, out_features=37, bias=True)\n",")"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","metadata":{"id":"R08BB9adUNI4"},"source":["def loss_fn(outputs, targets):\n","    return torch.nn.BCEWithLogitsLoss()(outputs, targets)\n","\n","optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cuRmU5SXXY2u"},"source":["val_targets=[]\n","val_outputs=[]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PN93i6WKVWn7"},"source":["def train_model(n_epochs, training_loader, validation_loader, model, \n","                optimizer):\n","   \n","  # initialize tracker for minimum validation loss\n","  valid_loss_min = np.Inf\n","   \n"," \n","  for epoch in range(1, n_epochs+1):\n","    train_loss = 0\n","    valid_loss = 0\n","\n","    model.train()\n","    print('############# Epoch {}: Training Start   #############'.format(epoch))\n","    for batch_idx, data in enumerate(training_loader):\n","        #print('yyy epoch', batch_idx)\n","        ids = data['input_ids'].to(device, dtype = torch.long)\n","        mask = data['attention_mask'].to(device, dtype = torch.long)\n","        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n","        targets = data['targets'].to(device, dtype = torch.float)\n","\n","        outputs = model(ids, mask, token_type_ids)\n","\n","        optimizer.zero_grad()\n","        loss = loss_fn(outputs, targets)\n","        #if batch_idx%5000==0:\n","         #   print(f'Epoch: {epoch}, Training Loss:  {loss.item()}')\n","        \n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        #print('before loss data in training', loss.item(), train_loss)\n","        train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.item() - train_loss))\n","        #print('after loss data in training', loss.item(), train_loss)\n","    \n","    print('############# Epoch {}: Training End     #############'.format(epoch))\n","    \n","    print('############# Epoch {}: Validation Start   #############'.format(epoch))\n","    ######################    \n","    # validate the model #\n","    ######################\n"," \n","    model.eval()\n","   \n","    with torch.no_grad():\n","      for batch_idx, data in enumerate(validation_loader, 0):\n","            ids = data['input_ids'].to(device, dtype = torch.long)\n","            mask = data['attention_mask'].to(device, dtype = torch.long)\n","            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n","            targets = data['targets'].to(device, dtype = torch.float)\n","            outputs = model(ids, mask, token_type_ids)\n","\n","            loss = loss_fn(outputs, targets)\n","            valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.item() - valid_loss))\n","            val_targets.extend(targets.cpu().detach().numpy().tolist())\n","            val_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n","\n","      print('############# Epoch {}: Validation End     #############'.format(epoch))\n","      '''# calculate average losses\n","      #print('before cal avg train loss', train_loss)\n","      train_loss = train_loss/len(training_loader)\n","      valid_loss = valid_loss/len(validation_loader)\n","      # print training/validation statistics \n","      print('Epoch: {} \\tAvgerage Training Loss: {:.6f} \\tAverage Validation Loss: {:.6f}'.format(\n","            epoch, \n","            train_loss,\n","            valid_loss\n","            ))\n","      \n","      # create checkpoint variable and add important data\n","      checkpoint = {\n","            'epoch': epoch + 1,\n","            'valid_loss_min': valid_loss,\n","            'state_dict': model.state_dict(),\n","            'optimizer': optimizer.state_dict()\n","      }\n","        \n","        # save checkpoint\n","      save_ckp(checkpoint, False, checkpoint_path, best_model_path)\n","        \n","      ## TODO: save the model if validation loss has decreased\n","      if valid_loss <= valid_loss_min:\n","        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,valid_loss))\n","        # save checkpoint as best model\n","        save_ckp(checkpoint, True, checkpoint_path, best_model_path)\n","        valid_loss_min = valid_loss\n","        '''\n","\n","    print('############# Epoch {}  Done   #############\\n'.format(epoch))\n","\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YgD-KcHfVWjS","outputId":"9427451a-c2b3-467f-d70b-31388e5c8955","executionInfo":{"status":"ok","timestamp":1669962286139,"user_tz":-330,"elapsed":1369918,"user":{"displayName":"Agrima Malhotra","userId":"02024143427081502811"}}},"source":["trained_model = train_model(EPOCHS, train_data_loader, val_data_loader, model, optimizer)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["############# Epoch 1: Training Start   #############\n","############# Epoch 1: Training End     #############\n","############# Epoch 1: Validation Start   #############\n","############# Epoch 1: Validation End     #############\n","############# Epoch 1  Done   #############\n","\n","############# Epoch 2: Training Start   #############\n","############# Epoch 2: Training End     #############\n","############# Epoch 2: Validation Start   #############\n","############# Epoch 2: Validation End     #############\n","############# Epoch 2  Done   #############\n","\n","############# Epoch 3: Training Start   #############\n","############# Epoch 3: Training End     #############\n","############# Epoch 3: Validation Start   #############\n","############# Epoch 3: Validation End     #############\n","############# Epoch 3  Done   #############\n","\n","############# Epoch 4: Training Start   #############\n","############# Epoch 4: Training End     #############\n","############# Epoch 4: Validation Start   #############\n","############# Epoch 4: Validation End     #############\n","############# Epoch 4  Done   #############\n","\n","############# Epoch 5: Training Start   #############\n","############# Epoch 5: Training End     #############\n","############# Epoch 5: Validation Start   #############\n","############# Epoch 5: Validation End     #############\n","############# Epoch 5  Done   #############\n","\n","############# Epoch 6: Training Start   #############\n","############# Epoch 6: Training End     #############\n","############# Epoch 6: Validation Start   #############\n","############# Epoch 6: Validation End     #############\n","############# Epoch 6  Done   #############\n","\n","############# Epoch 7: Training Start   #############\n","############# Epoch 7: Training End     #############\n","############# Epoch 7: Validation Start   #############\n","############# Epoch 7: Validation End     #############\n","############# Epoch 7  Done   #############\n","\n","############# Epoch 8: Training Start   #############\n","############# Epoch 8: Training End     #############\n","############# Epoch 8: Validation Start   #############\n","############# Epoch 8: Validation End     #############\n","############# Epoch 8  Done   #############\n","\n","############# Epoch 9: Training Start   #############\n","############# Epoch 9: Training End     #############\n","############# Epoch 9: Validation Start   #############\n","############# Epoch 9: Validation End     #############\n","############# Epoch 9  Done   #############\n","\n","############# Epoch 10: Training Start   #############\n","############# Epoch 10: Training End     #############\n","############# Epoch 10: Validation Start   #############\n","############# Epoch 10: Validation End     #############\n","############# Epoch 10  Done   #############\n","\n","############# Epoch 11: Training Start   #############\n","############# Epoch 11: Training End     #############\n","############# Epoch 11: Validation Start   #############\n","############# Epoch 11: Validation End     #############\n","############# Epoch 11  Done   #############\n","\n","############# Epoch 12: Training Start   #############\n","############# Epoch 12: Training End     #############\n","############# Epoch 12: Validation Start   #############\n","############# Epoch 12: Validation End     #############\n","############# Epoch 12  Done   #############\n","\n","############# Epoch 13: Training Start   #############\n","############# Epoch 13: Training End     #############\n","############# Epoch 13: Validation Start   #############\n","############# Epoch 13: Validation End     #############\n","############# Epoch 13  Done   #############\n","\n","############# Epoch 14: Training Start   #############\n","############# Epoch 14: Training End     #############\n","############# Epoch 14: Validation Start   #############\n","############# Epoch 14: Validation End     #############\n","############# Epoch 14  Done   #############\n","\n","############# Epoch 15: Training Start   #############\n","############# Epoch 15: Training End     #############\n","############# Epoch 15: Validation Start   #############\n","############# Epoch 15: Validation End     #############\n","############# Epoch 15  Done   #############\n","\n","############# Epoch 16: Training Start   #############\n","############# Epoch 16: Training End     #############\n","############# Epoch 16: Validation Start   #############\n","############# Epoch 16: Validation End     #############\n","############# Epoch 16  Done   #############\n","\n","############# Epoch 17: Training Start   #############\n","############# Epoch 17: Training End     #############\n","############# Epoch 17: Validation Start   #############\n","############# Epoch 17: Validation End     #############\n","############# Epoch 17  Done   #############\n","\n","############# Epoch 18: Training Start   #############\n","############# Epoch 18: Training End     #############\n","############# Epoch 18: Validation Start   #############\n","############# Epoch 18: Validation End     #############\n","############# Epoch 18  Done   #############\n","\n","############# Epoch 19: Training Start   #############\n","############# Epoch 19: Training End     #############\n","############# Epoch 19: Validation Start   #############\n","############# Epoch 19: Validation End     #############\n","############# Epoch 19  Done   #############\n","\n","############# Epoch 20: Training Start   #############\n","############# Epoch 20: Training End     #############\n","############# Epoch 20: Validation Start   #############\n","############# Epoch 20: Validation End     #############\n","############# Epoch 20  Done   #############\n","\n","############# Epoch 21: Training Start   #############\n","############# Epoch 21: Training End     #############\n","############# Epoch 21: Validation Start   #############\n","############# Epoch 21: Validation End     #############\n","############# Epoch 21  Done   #############\n","\n","############# Epoch 22: Training Start   #############\n","############# Epoch 22: Training End     #############\n","############# Epoch 22: Validation Start   #############\n","############# Epoch 22: Validation End     #############\n","############# Epoch 22  Done   #############\n","\n","############# Epoch 23: Training Start   #############\n","############# Epoch 23: Training End     #############\n","############# Epoch 23: Validation Start   #############\n","############# Epoch 23: Validation End     #############\n","############# Epoch 23  Done   #############\n","\n","############# Epoch 24: Training Start   #############\n","############# Epoch 24: Training End     #############\n","############# Epoch 24: Validation Start   #############\n","############# Epoch 24: Validation End     #############\n","############# Epoch 24  Done   #############\n","\n","############# Epoch 25: Training Start   #############\n","############# Epoch 25: Training End     #############\n","############# Epoch 25: Validation Start   #############\n","############# Epoch 25: Validation End     #############\n","############# Epoch 25  Done   #############\n","\n","############# Epoch 26: Training Start   #############\n","############# Epoch 26: Training End     #############\n","############# Epoch 26: Validation Start   #############\n","############# Epoch 26: Validation End     #############\n","############# Epoch 26  Done   #############\n","\n","############# Epoch 27: Training Start   #############\n","############# Epoch 27: Training End     #############\n","############# Epoch 27: Validation Start   #############\n","############# Epoch 27: Validation End     #############\n","############# Epoch 27  Done   #############\n","\n","############# Epoch 28: Training Start   #############\n","############# Epoch 28: Training End     #############\n","############# Epoch 28: Validation Start   #############\n","############# Epoch 28: Validation End     #############\n","############# Epoch 28  Done   #############\n","\n","############# Epoch 29: Training Start   #############\n","############# Epoch 29: Training End     #############\n","############# Epoch 29: Validation Start   #############\n","############# Epoch 29: Validation End     #############\n","############# Epoch 29  Done   #############\n","\n","############# Epoch 30: Training Start   #############\n","############# Epoch 30: Training End     #############\n","############# Epoch 30: Validation Start   #############\n","############# Epoch 30: Validation End     #############\n","############# Epoch 30  Done   #############\n","\n","############# Epoch 31: Training Start   #############\n","############# Epoch 31: Training End     #############\n","############# Epoch 31: Validation Start   #############\n","############# Epoch 31: Validation End     #############\n","############# Epoch 31  Done   #############\n","\n","############# Epoch 32: Training Start   #############\n","############# Epoch 32: Training End     #############\n","############# Epoch 32: Validation Start   #############\n","############# Epoch 32: Validation End     #############\n","############# Epoch 32  Done   #############\n","\n","############# Epoch 33: Training Start   #############\n","############# Epoch 33: Training End     #############\n","############# Epoch 33: Validation Start   #############\n","############# Epoch 33: Validation End     #############\n","############# Epoch 33  Done   #############\n","\n","############# Epoch 34: Training Start   #############\n","############# Epoch 34: Training End     #############\n","############# Epoch 34: Validation Start   #############\n","############# Epoch 34: Validation End     #############\n","############# Epoch 34  Done   #############\n","\n","############# Epoch 35: Training Start   #############\n","############# Epoch 35: Training End     #############\n","############# Epoch 35: Validation Start   #############\n","############# Epoch 35: Validation End     #############\n","############# Epoch 35  Done   #############\n","\n","############# Epoch 36: Training Start   #############\n","############# Epoch 36: Training End     #############\n","############# Epoch 36: Validation Start   #############\n","############# Epoch 36: Validation End     #############\n","############# Epoch 36  Done   #############\n","\n","############# Epoch 37: Training Start   #############\n","############# Epoch 37: Training End     #############\n","############# Epoch 37: Validation Start   #############\n","############# Epoch 37: Validation End     #############\n","############# Epoch 37  Done   #############\n","\n","############# Epoch 38: Training Start   #############\n","############# Epoch 38: Training End     #############\n","############# Epoch 38: Validation Start   #############\n","############# Epoch 38: Validation End     #############\n","############# Epoch 38  Done   #############\n","\n","############# Epoch 39: Training Start   #############\n","############# Epoch 39: Training End     #############\n","############# Epoch 39: Validation Start   #############\n","############# Epoch 39: Validation End     #############\n","############# Epoch 39  Done   #############\n","\n","############# Epoch 40: Training Start   #############\n","############# Epoch 40: Training End     #############\n","############# Epoch 40: Validation Start   #############\n","############# Epoch 40: Validation End     #############\n","############# Epoch 40  Done   #############\n","\n","############# Epoch 41: Training Start   #############\n","############# Epoch 41: Training End     #############\n","############# Epoch 41: Validation Start   #############\n","############# Epoch 41: Validation End     #############\n","############# Epoch 41  Done   #############\n","\n","############# Epoch 42: Training Start   #############\n","############# Epoch 42: Training End     #############\n","############# Epoch 42: Validation Start   #############\n","############# Epoch 42: Validation End     #############\n","############# Epoch 42  Done   #############\n","\n","############# Epoch 43: Training Start   #############\n","############# Epoch 43: Training End     #############\n","############# Epoch 43: Validation Start   #############\n","############# Epoch 43: Validation End     #############\n","############# Epoch 43  Done   #############\n","\n","############# Epoch 44: Training Start   #############\n","############# Epoch 44: Training End     #############\n","############# Epoch 44: Validation Start   #############\n","############# Epoch 44: Validation End     #############\n","############# Epoch 44  Done   #############\n","\n","############# Epoch 45: Training Start   #############\n","############# Epoch 45: Training End     #############\n","############# Epoch 45: Validation Start   #############\n","############# Epoch 45: Validation End     #############\n","############# Epoch 45  Done   #############\n","\n","############# Epoch 46: Training Start   #############\n","############# Epoch 46: Training End     #############\n","############# Epoch 46: Validation Start   #############\n","############# Epoch 46: Validation End     #############\n","############# Epoch 46  Done   #############\n","\n","############# Epoch 47: Training Start   #############\n","############# Epoch 47: Training End     #############\n","############# Epoch 47: Validation Start   #############\n","############# Epoch 47: Validation End     #############\n","############# Epoch 47  Done   #############\n","\n","############# Epoch 48: Training Start   #############\n","############# Epoch 48: Training End     #############\n","############# Epoch 48: Validation Start   #############\n","############# Epoch 48: Validation End     #############\n","############# Epoch 48  Done   #############\n","\n","############# Epoch 49: Training Start   #############\n","############# Epoch 49: Training End     #############\n","############# Epoch 49: Validation Start   #############\n","############# Epoch 49: Validation End     #############\n","############# Epoch 49  Done   #############\n","\n","############# Epoch 50: Training Start   #############\n","############# Epoch 50: Training End     #############\n","############# Epoch 50: Validation Start   #############\n","############# Epoch 50: Validation End     #############\n","############# Epoch 50  Done   #############\n","\n"]}]},{"cell_type":"code","metadata":{"id":"8L9o8grxVWgc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669963007048,"user_tz":-330,"elapsed":898,"user":{"displayName":"Agrima Malhotra","userId":"02024143427081502811"}},"outputId":"fcb9716a-9385-413c-d462-f5c4df9c8157"},"source":["# testing\n","example = 'It is not disputed before us that the requisite ceremonies for a valid marriage under the personal law of the parties have been gone into in this case, although the marriage is null and void by reason of the provisions of sections 5 and 11 of the Hindu Marriage Act, 1955. This Act was passed with the object of amending and modifying the law relating to marriages among Hindus. Under the customary law, there was no restriction for a male Hindu to marry more than one women. This right of the Hindu husband under the customary Hindu Law was curtailed for the first time in the then Bombay Province by enacting the Bombay Prevention of Hindu Bigamous Marriage Act, 1946, provisions whereof declared bigamy to be illegal. Under the provisions of the said Act and other similar laws enacted by other Provincial Legislatures, a second marriage by a Hindu person during the life time of the spouse was declared illegal and going through such a marriage was made penal. Consistent with the object of codifying the marriage laws amongst Hindus, all these State laws were repealed by the Hindu Marriage Act, 1955. Section 5 of the Hindu Marriage Act provides for the conditions for solemnisation of marriage between any two Hindus. Section 11 declares that a marriage solemnised after the commencement of the Act shall be null and void if it contravenes any of conditions specified in Clause (i), (iv) and (v) of section 5. One of the conditions for a marriage as required by section 5 is that neither party has a spouse living at the time of the marriage, and this is condition No. (i) in section 5. Section 11 also gives a remedy to either party to the marriage to file a petition for a declaration by a decree of nullity of marriage on any one of the said three conditions of section 5 being shown to the have been contravened. Obviously, the second marriage in such circumstances being void, it cannot create a legal status of husband and wife between the parties. It is true that section 11 also gives a right to the parties to the marriage to file a petition for a declaration of nullity by a decree of the Court , but the filing of the petition or passing the decree is not a condition precedent for putting an end to the marriage. What ultimately is declared on such a petition is nothing but the status of the party as on the date of the marriage, and therefore, the marriage does not continue to remain valid until a decree is passed. What is null and void cannot be deemed to be in existence for any purposes whatsoever. Under the circumstances, if a marriage is solemnised in contravention of any of the said three conditions referred to in section 5(i), the woman cannot get the status of the wife nor the male gets the status of a husband qua her. The second marriage does not continue to be valid till the passing of the decree for a nullity. The position is also clear from the fact that bigamy is made penal by section 17 of the Hindu Marriage Act which provides that any marriage between two Hindu solemnised after the commencement of this Act is void if at the date of such marriage either party had a husband or wife living and the provisions of sections 494 and 495 of the Indian Penal Code shall apply accordingly. The position is made further clear from the anxiety of the legislature to protect children of such a marriage by providing in section 16 that not withstanding that a marriage is null and void under section 11, any child of such marriage who would have been legitimate if the marriage had been valid, shall be legitimate, whether such child is born before or after the commencement of the Marriage Laws (Amendment) Act, 1976, and whether or not a decree of nullity is granted in respect of that marriage under this Act and whether or not the marriage is held to be void otherwise than on a petition under this Act. However, the rights of such a child are somewhat curtailed in the matter of inheritance to the property, because sub-section (3) of section 16 says that a child of such a marriage would not be entitled to any rights in or to the property of any person other than the parents. Having regard to all these provisions, the marriage of the petitioner with the respondent was void ab initio and the respondent could not get the status of a legally wedded wife inspite of the solemnisation of the marriage under the Hindu Law having gone into. Indeed, Mr. Gavnekar did not dispute this legal position. He, however, contended that the provisions of section 25 of the Hindu Marriage Act conferred a right of maintenance on the second wife and the word \"wife\" in section 125 of the Code of Criminal Procedure will have to be given a wider meaning as including a Hindu wife whose marriage may be otherwise void.'\n","example=pre_process(example)\n","encodings = tokenizer.encode_plus(\n","    example,\n","    None,\n","    add_special_tokens=True,\n","    max_length=MAX_LEN,\n","    padding='max_length',\n","    return_token_type_ids=True,\n","    truncation=True,\n","    return_attention_mask=True,\n","    return_tensors='pt'\n",")\n","model.eval()\n","with torch.no_grad():\n","    input_ids = encodings['input_ids'].to(device, dtype=torch.long)\n","    attention_mask = encodings['attention_mask'].to(device, dtype=torch.long)\n","    token_type_ids = encodings['token_type_ids'].to(device, dtype=torch.long)\n","    output = model(input_ids, attention_mask, token_type_ids)\n","    final_output = torch.sigmoid(output).cpu().detach().numpy().tolist()\n","    output=np.flip(np.argsort(final_output,axis=1)[0][-5:])\n","    for i in output:\n","      print(\"Section \"+ train_df.columns[1:].to_list()[i])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["11\n","5\n","25\n","16\n","3\n"]}]}]}